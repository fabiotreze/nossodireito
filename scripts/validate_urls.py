#!/usr/bin/env python3
"""
NossoDireito ‚Äî Validador de URLs e Fontes Oficiais
====================================================
Valida que TODOS os links em direitos.json pertencem a dom√≠nios
oficiais do governo brasileiro (.gov.br, .leg.br, .jus.br, .def.br,
.mp.br, .mil.br).

Tamb√©m pode verificar acessibilidade HTTP (status 200) com --check-live.

Uso:
    python scripts/validate_urls.py                 # Valida√ß√£o de dom√≠nios
    python scripts/validate_urls.py --check-live    # + verifica HTTP status
    python scripts/validate_urls.py --fix           # Sugere corre√ß√µes
    python scripts/validate_urls.py --dict          # Valida dicion√°rio tamb√©m

Retorna exit code 0 se tudo OK, 1 se h√° URLs n√£o-governamentais.
"""

import json
import sys
import time
import urllib.error
import urllib.parse
import urllib.request
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
DATA_FILE = ROOT / "data" / "direitos.json"
DICT_FILE = ROOT / "data" / "dicionario_pcd.json"

# Dom√≠nios oficiais aceitos
DOMINIOS_OFICIAIS = (
    ".gov.br",
    ".leg.br",
    ".jus.br",
    ".mp.br",
    ".def.br",
    ".mil.br",
)

# Dom√≠nios internacionais confi√°veis (organismos oficiais)
DOMINIOS_INTERNACIONAIS = (
    "icd.who.int",  # OMS ‚Äî Classifica√ß√£o Internacional de Doen√ßas
)

# Esquemas aceitos (al√©m de https://)
ESQUEMAS_ACEITOS = ("https", "http")

USER_AGENT = "NossoDireito-URLValidator/1.0 (+https://nossodireito.fabiotreze.com)"


def is_official_url(url: str) -> bool:
    """Verifica se URL pertence a dom√≠nio oficial do governo brasileiro ou organismo internacional confi√°vel."""
    try:
        parsed = urllib.parse.urlparse(url)
        if parsed.scheme not in ESQUEMAS_ACEITOS:
            return False
        domain = (parsed.hostname or "").lower()
        if any(domain.endswith(d) for d in DOMINIOS_OFICIAIS):
            return True
        if any(domain == d or domain.endswith("." + d) for d in DOMINIOS_INTERNACIONAIS):
            return True
        return False
    except Exception:
        return False


def extract_all_urls(data: dict) -> list[dict]:
    """Extrai todas as URLs de direitos.json com contexto."""
    urls = []

    def _add(url, context, path):
        if url and isinstance(url, str) and url.startswith("http"):
            urls.append({"url": url, "context": context, "path": path})

    # Fontes
    for i, fonte in enumerate(data.get("fontes", [])):
        _add(fonte.get("url"), f"fonte: {fonte.get('nome', '?')}", f"fontes[{i}].url")
        for sub in fonte.get("servicos", []):
            if isinstance(sub, dict):
                _add(sub.get("url"), f"fonte.servi√ßo: {sub.get('nome', '?')}", f"fontes[{i}].servicos")

    # Categorias
    for i, cat in enumerate(data.get("categorias", [])):
        cat_id = cat.get("id", "?")
        # Links
        for j, link in enumerate(cat.get("links", [])):
            _add(link.get("url"), f"[{cat_id}] link: {link.get('titulo', '?')}", f"categorias[{i}].links[{j}]")
        # Base legal
        for j, bl in enumerate(cat.get("base_legal", [])):
            _add(bl.get("link"), f"[{cat_id}] lei: {bl.get('lei', '?')}", f"categorias[{i}].base_legal[{j}]")

    # Institui√ß√µes de apoio
    for i, inst in enumerate(data.get("instituicoes_apoio", [])):
        _add(inst.get("url"), f"institui√ß√£o: {inst.get('nome', '?')}", f"instituicoes_apoio[{i}].url")
        _add(inst.get("contato"), f"institui√ß√£o contato: {inst.get('nome', '?')}", f"instituicoes_apoio[{i}].contato")

    # √ìrg√£os estaduais (expandidos: url, sefaz, detran)
    for i, uf_data in enumerate(data.get("orgaos_estaduais", [])):
        uf = uf_data.get("uf", "?")
        _add(uf_data.get("url"), f"√≥rg√£o estadual: {uf}", f"orgaos_estaduais[{i}].url")
        _add(uf_data.get("sefaz"), f"SEFAZ estadual: {uf}", f"orgaos_estaduais[{i}].sefaz")
        _add(uf_data.get("detran"), f"DETRAN estadual: {uf}", f"orgaos_estaduais[{i}].detran")

    # IPVA SEFAZ URLs (inline em isencoes_tributarias)
    isencoes = next((c for c in data.get("categorias", []) if c.get("id") == "isencoes_tributarias"), None)
    if isencoes:
        for i, est in enumerate(isencoes.get("ipva_estados", [])):
            uf = est.get("uf", "?")
            _add(est.get("sefaz"), f"IPVA SEFAZ: {uf}", f"isencoes_tributarias.ipva_estados[{i}].sefaz")
        for i, est in enumerate(isencoes.get("ipva_estados_detalhado", [])):
            uf = est.get("uf", "?")
            _add(est.get("sefaz"), f"IPVA SEFAZ detalhado: {uf}", f"isencoes_tributarias.ipva_estados_detalhado[{i}].sefaz")

    return urls


MAX_RETRIES = 3
RETRY_DELAY = 4  # seconds


def check_url_live(url: str) -> tuple[int, str]:
    """Verifica se URL responde (status 200).

    Inclui retry com backoff exponencial e fallback HEAD‚ÜíGET.

    Returns (status_code, mensagem).
    """
    last_exc: Exception | None = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            req = urllib.request.Request(url, method="HEAD", headers={
                "User-Agent": USER_AGENT,
            })
            with urllib.request.urlopen(req, timeout=15) as resp:
                return resp.status, "OK"
        except urllib.error.HTTPError as e:
            if e.code in (405, 403):
                # Server rejects HEAD ‚Äî try GET
                try:
                    req2 = urllib.request.Request(url, method="GET", headers={
                        "User-Agent": USER_AGENT,
                    })
                    with urllib.request.urlopen(req2, timeout=15) as resp2:
                        return resp2.status, "OK"
                except urllib.error.HTTPError as e2:
                    if e2.code >= 500 and attempt < MAX_RETRIES:
                        last_exc = e2
                        time.sleep(RETRY_DELAY * attempt)
                        continue
                    return e2.code, f"HTTP {e2.code}"
                except (TimeoutError, OSError, ConnectionError) as e2:
                    last_exc = e2
                    if attempt < MAX_RETRIES:
                        time.sleep(RETRY_DELAY * attempt)
                        continue
                    return 0, f"Erro: {e2}"
            elif e.code >= 500 and attempt < MAX_RETRIES:
                last_exc = e
                time.sleep(RETRY_DELAY * attempt)
                continue
            else:
                return e.code, f"HTTP {e.code}"
        except urllib.error.URLError as e:
            last_exc = e
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY * attempt)
                continue
            return 0, f"Erro: {e.reason}"
        except (TimeoutError, OSError, ConnectionError) as e:
            last_exc = e
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY * attempt)
                continue
            return 0, f"Erro: {e}"
    return 0, f"Erro: {last_exc}"


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Valida URLs em direitos.json")
    parser.add_argument("--check-live", action="store_true",
                        help="Verificar status HTTP de cada URL")
    parser.add_argument("--dict", action="store_true",
                        help="Validar dicion√°rio PcD tamb√©m")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Mostrar todas as URLs (n√£o apenas erros)")
    args = parser.parse_args()

    print("=" * 60)
    print("üîç VALIDA√á√ÉO DE URLs ‚Äî NOSSODIREITO")
    print("=" * 60)

    # 1. Carregar direitos.json
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    urls = extract_all_urls(data)
    print(f"\nüìä Total de URLs extra√≠das: {len(urls)}")

    # 2. Validar dom√≠nios
    non_gov = []
    gov_count = 0
    for entry in urls:
        if is_official_url(entry["url"]):
            gov_count += 1
            if args.verbose:
                print(f"  ‚úÖ {entry['url'][:80]}")
        else:
            non_gov.append(entry)

    print(f"\n{'‚úÖ' if not non_gov else '‚ùå'} Dom√≠nios oficiais: {gov_count}/{len(urls)}")

    if non_gov:
        print(f"\n‚ö†Ô∏è  URLs N√ÉO-GOVERNAMENTAIS ({len(non_gov)}):")
        for entry in non_gov:
            print(f"  ‚ùå {entry['url']}")
            print(f"     üìç {entry['context']} ({entry['path']})")

    # 3. Verificar URLs duplicadas
    seen = {}
    duplicates = []
    for entry in urls:
        url_clean = entry["url"].rstrip("/")
        if url_clean in seen:
            duplicates.append((entry, seen[url_clean]))
        else:
            seen[url_clean] = entry

    if duplicates:
        print(f"\n‚ö†Ô∏è  URLs DUPLICADAS ({len(duplicates)}):")
        for dup, orig in duplicates[:10]:
            print(f"  üîÑ {dup['url'][:60]}")
            print(f"     1¬™: {orig['context']}")
            print(f"     2¬™: {dup['context']}")

    # 4. Verificar dicion√°rio
    if args.dict and DICT_FILE.exists():
        print(f"\nüìñ Validando dicion√°rio PcD...")
        with open(DICT_FILE, "r", encoding="utf-8") as f:
            dicio = json.load(f)
        dict_urls = []
        for lei in dicio.get("leis", []):
            if lei.get("url"):
                dict_urls.append({"url": lei["url"], "context": f"lei: {lei.get('nome', '?')}", "path": "leis"})
        for b in dicio.get("beneficios", []):
            if b.get("url"):
                dict_urls.append({"url": b["url"], "context": f"benef√≠cio: {b.get('nome', '?')}", "path": "beneficios"})
        for o in dicio.get("orgaos_denuncia", []):
            if o.get("url"):
                dict_urls.append({"url": o["url"], "context": f"√≥rg√£o: {o.get('nome', '?')}", "path": "orgaos_denuncia"})

        dict_non_gov = [e for e in dict_urls if not is_official_url(e["url"])]
        print(f"  URLs no dicion√°rio: {len(dict_urls)}")
        print(f"  {'‚úÖ' if not dict_non_gov else '‚ùå'} Oficiais: {len(dict_urls) - len(dict_non_gov)}/{len(dict_urls)}")
        if dict_non_gov:
            for entry in dict_non_gov:
                print(f"  ‚ùå {entry['url']} ({entry['context']})")
            non_gov.extend(dict_non_gov)

    # 5. Check live (opcional)
    if args.check_live:
        print(f"\nüåê Verificando acessibilidade HTTP ({len(urls)} URLs)...")
        errors = []
        for i, entry in enumerate(urls):
            status, msg = check_url_live(entry["url"])
            if status == 0 or status >= 400:
                errors.append((entry, status, msg))
                print(f"  ‚ùå [{status}] {entry['url'][:60]} ‚Äî {msg}")
            elif args.verbose:
                print(f"  ‚úÖ [{status}] {entry['url'][:60]}")
            time.sleep(0.3)
            if (i + 1) % 20 == 0:
                print(f"  ... {i + 1}/{len(urls)} verificadas")

        print(f"\n{'‚úÖ' if not errors else '‚ö†Ô∏è'} HTTP acess√≠veis: {len(urls) - len(errors)}/{len(urls)}")
        if errors:
            print(f"  {len(errors)} URLs com problema de acesso")

    # 6. Resumo final
    print("\n" + "=" * 60)
    if not non_gov:
        print("‚úÖ RESULTADO: TODAS AS URLs S√ÉO DE FONTES OFICIAIS")
        print("   Transpar√™ncia total ‚Äî nenhum site externo ao governo.")
        sys.exit(0)
    else:
        print(f"‚ùå RESULTADO: {len(non_gov)} URL(s) N√ÉO-GOVERNAMENTAL(IS)")
        print("   Corrija antes de publicar. Apenas .gov.br, .leg.br, .jus.br, .def.br, .mp.br, .mil.br.")
        sys.exit(1)


if __name__ == "__main__":
    main()
