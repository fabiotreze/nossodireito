#!/usr/bin/env python3
"""
NossoDireito ‚Äî Descoberta Autom√°tica de Novos Benef√≠cios PcD
=============================================================
Monitora fontes oficiais brasileiras (DOU, gov.br, Senado, LexML)
para detectar novos direitos, benef√≠cios, leis e servi√ßos para PcD.

Gera um relat√≥rio JSON + Markdown com candidatos para an√°lise humana.

Uso:
    python scripts/discover_benefits.py                       # Todos os n√≠veis
    python scripts/discover_benefits.py --dry-run             # Apenas mostra
    python scripts/discover_benefits.py --since 30            # √öltimos 30 dias
    python scripts/discover_benefits.py --nivel federal       # S√≥ federal
    python scripts/discover_benefits.py --nivel estadual      # S√≥ 27 estados
    python scripts/discover_benefits.py --nivel municipal     # S√≥ munic√≠pios
    python scripts/discover_benefits.py --estado SP RJ        # Filtrar UFs
    python scripts/discover_benefits.py --cidade barueri-sp   # Filtrar cidade

Fontes consultadas:
  Federal:
    1. gov.br @@search (DOU, Not√≠cias, Servi√ßos) ‚Äî busca universal
    2. Senado Federal Dados Abertos ‚Äî legisla√ß√£o em tramita√ß√£o
    3. Portal gov.br Servi√ßos ‚Äî cat√°logo de servi√ßos digitais PcD
    4. Portal gov.br Not√≠cias ‚Äî INSS, MDS, Sa√∫de, Direitos Humanos
    5. LexML Brasil ‚Äî metadados legislativos abertos
  Estadual:
    6. gov.br @@search por estado ‚Äî 27 UFs (todos os estados + DF)
  Municipal:
    7. gov.br @@search por munic√≠pio ‚Äî cidades configuradas
    8. Portais municipais oficiais ‚Äî scraping de p√°ginas PcD

Cobertura: Brasil inteiro (federal + 27 estados + munic√≠pios).
Apenas fontes oficiais (.gov.br, .leg.br, .jus.br, .mp.br, .def.br).

Nenhum dado √© alterado automaticamente. O script gera candidatos para
revis√£o humana seguindo o fluxo definido em GOVERNANCE.md.
"""

import argparse
import hashlib
import json
import os
import re
import sys
import time
import urllib.error
import urllib.parse
import urllib.request
from datetime import datetime, timedelta
from pathlib import Path

# ‚îÄ‚îÄ‚îÄ Configura√ß√£o ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROOT = Path(__file__).resolve().parent.parent
DATA_FILE = ROOT / "data" / "direitos.json"
DICT_FILE = ROOT / "data" / "dicionario_pcd.json"
REPORT_DIR = ROOT / "data"
REPORT_JSON = REPORT_DIR / "discovery_report.json"
REPORT_MD = REPORT_DIR / "discovery_report.md"

RATE_LIMIT_DELAY = 0.5  # segundos entre requisi√ß√µes
REQUEST_TIMEOUT = 15     # segundos por requisi√ß√£o
USER_AGENT = "NossoDireito-DiscoveryBot/1.0 (+https://nossodireito.fabiotreze.com)"

# Palavras-chave PcD para busca em fontes oficiais
PCD_KEYWORDS = [
    "pessoa com defici√™ncia",
    "pessoa com deficiencia",
    "pessoas com defici√™ncia",
    "pessoas com deficiencia",
    "PcD",
    "autismo",
    "TEA",
    "espectro autista",
    "transtorno do espectro autista",
    "defici√™ncia f√≠sica",
    "defici√™ncia visual",
    "defici√™ncia auditiva",
    "defici√™ncia intelectual",
    "defici√™ncia mental",
    "defici√™ncia m√∫ltipla",
    "acessibilidade",
    "inclus√£o social",
    "tecnologia assistiva",
    "benef√≠cio assistencial",
    "BPC",
    "LOAS",
    "Estatuto da Pessoa com Defici√™ncia",
    "Lei Brasileira de Inclus√£o",
    "CIPTEA",
    "Romeo Mion",
    "Berenice Piana",
    "Novo Viver sem Limite",
    "isen√ß√£o IPI defici√™ncia",
    "isen√ß√£o ICMS defici√™ncia",
    "isen√ß√£o IOF defici√™ncia",
    "passe livre interestadual",
    "meia-entrada defici√™ncia",
    "cota defici√™ncia",
    "reserva de vagas defici√™ncia",
    "aposentadoria pessoa com defici√™ncia",
    "FGTS defici√™ncia",
    "plano de sa√∫de defici√™ncia",
    "SUS terapias",
    "atendimento priorit√°rio defici√™ncia",
]

# Termos de busca reduzidos para consultas a APIs (evita excesso de requests)
SEARCH_QUERIES = [
    "pessoa com defici√™ncia",
    "autismo TEA",
    "acessibilidade inclus√£o",
    "BPC LOAS defici√™ncia",
    "isen√ß√£o defici√™ncia",
    "tecnologia assistiva",
]

# IDs j√° conhecidos em direitos.json (carregados dinamicamente)
KNOWN_IDS: set[str] = set()
KNOWN_URLS: set[str] = set()
KNOWN_LAWS: set[str] = set()

# ‚îÄ‚îÄ‚îÄ Configura√ß√£o Geogr√°fica (Brasil inteiro) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Todos os 27 estados brasileiros (26 estados + DF)
UF_ESTADOS = {
    "AC": "Acre", "AL": "Alagoas", "AP": "Amap√°", "AM": "Amazonas",
    "BA": "Bahia", "CE": "Cear√°", "DF": "Distrito Federal",
    "ES": "Esp√≠rito Santo", "GO": "Goi√°s", "MA": "Maranh√£o",
    "MT": "Mato Grosso", "MS": "Mato Grosso do Sul", "MG": "Minas Gerais",
    "PA": "Par√°", "PB": "Para√≠ba", "PR": "Paran√°", "PE": "Pernambuco",
    "PI": "Piau√≠", "RJ": "Rio de Janeiro", "RN": "Rio Grande do Norte",
    "RS": "Rio Grande do Sul", "RO": "Rond√¥nia", "RR": "Roraima",
    "SC": "Santa Catarina", "SP": "S√£o Paulo", "SE": "Sergipe",
    "TO": "Tocantins",
}

# Munic√≠pios com portais oficiais configurados (expand√≠vel)
# Para adicionar cidades, basta inserir uma entrada aqui.
MUNICIPIOS_BR = {
    # Grande S√£o Paulo (solicitadas pelo usu√°rio)
    "barueri-sp": {
        "nome": "Barueri", "uf": "SP",
        "portal": "https://portal.barueri.sp.gov.br",
        "paginas_pcd": [
            "https://portal.barueri.sp.gov.br/cidadao/pessoa-com-deficiencia",
            "https://portal.barueri.sp.gov.br/secretarias/secretaria-dos-direitos-da-pessoa-com-deficiencia",
        ],
    },
    "osasco-sp": {
        "nome": "Osasco", "uf": "SP",
        "portal": "https://osasco.sp.gov.br",
        "paginas_pcd": [],
    },
    "santo-andre-sp": {
        "nome": "Santo Andr√©", "uf": "SP",
        "portal": "https://web.santoandre.sp.gov.br",
        "paginas_pcd": [
            "https://web.santoandre.sp.gov.br/portal/secretarias-paginas/320/inclusao-e-acessibilidade/",
        ],
    },
    # Capitais estaduais (portais .gov.br)
    "sao-paulo-sp": {
        "nome": "S√£o Paulo", "uf": "SP",
        "portal": "https://capital.sp.gov.br",
        "paginas_pcd": [
            "https://prefeitura.sp.gov.br/web/pessoa_com_deficiencia",
        ],
    },
    "belo-horizonte-mg": {
        "nome": "Belo Horizonte", "uf": "MG",
        "portal": "https://prefeitura.pbh.gov.br",
        "paginas_pcd": [],
    },
    "salvador-ba": {
        "nome": "Salvador", "uf": "BA",
        "portal": "https://www.salvador.ba.gov.br",
        "paginas_pcd": [],
    },
    "brasilia-df": {
        "nome": "Bras√≠lia", "uf": "DF",
        "portal": "https://www.df.gov.br",
        "paginas_pcd": [],
    },
    "curitiba-pr": {
        "nome": "Curitiba", "uf": "PR",
        "portal": "https://www.curitiba.pr.gov.br",
        "paginas_pcd": [],
    },
    "recife-pe": {
        "nome": "Recife", "uf": "PE",
        "portal": "https://www2.recife.pe.gov.br",
        "paginas_pcd": [],
    },
    "fortaleza-ce": {
        "nome": "Fortaleza", "uf": "CE",
        "portal": "https://www.fortaleza.ce.gov.br",
        "paginas_pcd": [],
    },
    "manaus-am": {
        "nome": "Manaus", "uf": "AM",
        "portal": "https://www.manaus.am.gov.br",
        "paginas_pcd": [],
    },
    "belem-pa": {
        "nome": "Bel√©m", "uf": "PA",
        "portal": "https://www.belem.pa.gov.br",
        "paginas_pcd": [],
    },
    "goiania-go": {
        "nome": "Goi√¢nia", "uf": "GO",
        "portal": "https://www.goiania.go.gov.br",
        "paginas_pcd": [],
    },
    "guarulhos-sp": {
        "nome": "Guarulhos", "uf": "SP",
        "portal": "https://www.guarulhos.sp.gov.br",
        "paginas_pcd": [],
    },
    "campinas-sp": {
        "nome": "Campinas", "uf": "SP",
        "portal": "https://www.campinas.sp.gov.br",
        "paginas_pcd": [],
    },
}

# Dom√≠nios oficiais do governo brasileiro (apenas estes s√£o aceitos)
DOMINIOS_OFICIAIS = (
    ".gov.br",
    ".leg.br",
    ".jus.br",
    ".mp.br",
    ".def.br",
    ".mil.br",
)

# Dom√≠nios internacionais confi√°veis (organismos oficiais)
DOMINIOS_INTERNACIONAIS = (
    "icd.who.int",  # OMS ‚Äî Classifica√ß√£o Internacional de Doen√ßas
)


def load_dictionary_keywords():
    """Carrega dicionario_pcd.json e enriquece PCD_KEYWORDS e SEARCH_QUERIES."""
    global PCD_KEYWORDS, SEARCH_QUERIES
    if not DICT_FILE.exists():
        return
    try:
        with open(DICT_FILE, "r", encoding="utf-8") as f:
            dicio = json.load(f)
        extra_kw = set()
        # Keywords de defici√™ncias
        for d in dicio.get("deficiencias", []):
            extra_kw.update(d.get("keywords_busca", []))
            extra_kw.update(d.get("sinonimos", []))
        # Keywords mestre
        for cat_kws in dicio.get("keywords_master", {}).values():
            if isinstance(cat_kws, list):
                extra_kw.update(cat_kws)
        # Keywords de benef√≠cios
        for b in dicio.get("beneficios", []):
            extra_kw.update(b.get("keywords_busca", []))
        # Adiciona novas keywords (sem duplicar)
        existing = {kw.lower() for kw in PCD_KEYWORDS}
        for kw in sorted(extra_kw):
            if kw.lower() not in existing and len(kw) >= 3:
                PCD_KEYWORDS.append(kw)
                existing.add(kw.lower())
        # Enriquece SEARCH_QUERIES com termos do dicion√°rio
        extra_queries = [
            "S√≠ndrome de Down defici√™ncia",
            "TDAH defici√™ncia escola",
            "paralisia cerebral reabilita√ß√£o",
            "nanismo acondroplasia defici√™ncia",
            "surdez implante coclear Libras",
            "cegueira bengala c√£o-guia",
            "esquizofrenia CAPS sa√∫de mental",
            "microcefalia Zika defici√™ncia",
            "pr√≥tese √≥rtese SUS",
            "cadeira de rodas motorizada",
        ]
        eq_set = {q.lower() for q in SEARCH_QUERIES}
        for q in extra_queries:
            if q.lower() not in eq_set:
                SEARCH_QUERIES.append(q)
                eq_set.add(q.lower())
    except (json.JSONDecodeError, KeyError, OSError) as e:
        print(f"‚ö†Ô∏è  Aviso: n√£o foi poss√≠vel carregar dicion√°rio PcD: {e}")


# Carrega dicion√°rio ao importar o m√≥dulo
load_dictionary_keywords()


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def log(msg: str, level: str = "INFO") -> None:
    symbols = {"INFO": "‚ÑπÔ∏è", "OK": "‚úÖ", "WARN": "‚ö†Ô∏è", "ERR": "‚ùå", "NEW": "üÜï", "SKIP": "‚è≠Ô∏è"}
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {symbols.get(level, '‚ÑπÔ∏è')}  {msg}")


def fetch(url: str, *, json_resp: bool = False, retries: int = 2) -> str | dict | None:
    """Faz GET com retry, rate-limit e User-Agent. Retorna texto ou dict."""
    headers = {"User-Agent": USER_AGENT, "Accept-Language": "pt-BR,pt;q=0.9"}
    if json_resp:
        headers["Accept"] = "application/json"

    for attempt in range(retries + 1):
        try:
            if attempt > 0:
                wait = 2 ** attempt
                log(f"  Retry {attempt}/{retries} em {wait}s...", "WARN")
                time.sleep(wait)

            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as resp:
                data = resp.read().decode("utf-8", errors="replace")
                time.sleep(RATE_LIMIT_DELAY)
                if json_resp:
                    return json.loads(data)
                return data
        except (urllib.error.URLError, urllib.error.HTTPError, json.JSONDecodeError, TimeoutError) as e:
            if attempt == retries:
                log(f"  Falha ao acessar {url[:80]}: {e}", "ERR")
                return None
    return None


def normalize_text(text: str) -> str:
    """Remove acentos e normaliza para compara√ß√£o."""
    import unicodedata
    nfkd = unicodedata.normalize("NFKD", text.lower())
    return "".join(c for c in nfkd if not unicodedata.combining(c))


def text_has_pcd_keywords(text: str) -> list[str]:
    """Retorna quais keywords PcD est√£o presentes no texto."""
    text_lower = text.lower()
    text_norm = normalize_text(text)
    found = []
    for kw in PCD_KEYWORDS:
        kw_lower = kw.lower()
        kw_norm = normalize_text(kw)
        # Para keywords curtas (TEA, BPC, PcD), usa word boundary
        # para evitar falsos positivos (ex: "teatro" ‚â† "TEA")
        if len(kw) <= 4:
            pattern = r'\b' + re.escape(kw_lower) + r'\b'
            pattern_norm = r'\b' + re.escape(kw_norm) + r'\b'
            if re.search(pattern, text_lower) or re.search(pattern_norm, text_norm):
                found.append(kw)
        else:
            if kw_lower in text_lower or kw_norm in text_norm:
                found.append(kw)
    return found


def is_known_url(url: str) -> bool:
    """Verifica se URL j√° est√° em direitos.json."""
    return url in KNOWN_URLS or url.rstrip("/") in KNOWN_URLS


def is_known_law(law_ref: str) -> bool:
    """Verifica se refer√™ncia legislativa j√° est√° em direitos.json."""
    # Normaliza: remove pontos do n√∫mero
    law_norm = re.sub(r"(\d)\.(\d)", r"\1\2", law_ref.lower().strip())
    return any(law_norm in kl for kl in KNOWN_LAWS)


def content_hash(text: str) -> str:
    return hashlib.sha256(text.encode()).hexdigest()[:12]


def is_official_domain(url: str) -> bool:
    """Verifica se URL pertence a dom√≠nio oficial do governo brasileiro ou organismo internacional confi√°vel."""
    try:
        domain = urllib.parse.urlparse(url).hostname or ""
        domain = domain.lower()
        if any(domain.endswith(d) for d in DOMINIOS_OFICIAIS):
            return True
        if any(domain == d or domain.endswith("." + d) for d in DOMINIOS_INTERNACIONAIS):
            return True
        return False
    except Exception:
        return False


# ‚îÄ‚îÄ‚îÄ Carregamento dos dados existentes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def load_existing_data() -> dict:
    """Carrega direitos.json e popula KNOWN_IDS, KNOWN_URLS, KNOWN_LAWS."""
    global KNOWN_IDS, KNOWN_URLS, KNOWN_LAWS

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    # IDs de categorias
    KNOWN_IDS = {cat["id"] for cat in data.get("categorias", [])}

    # Todas as URLs referenciadas
    urls = set()
    for fonte in data.get("fontes", []):
        if fonte.get("url"):
            urls.add(fonte["url"])
            urls.add(fonte["url"].rstrip("/"))
    for cat in data.get("categorias", []):
        for link in cat.get("links", []):
            if link.get("url"):
                urls.add(link["url"])
                urls.add(link["url"].rstrip("/"))
        for bl in cat.get("base_legal", []):
            if bl.get("link"):
                urls.add(bl["link"])
                urls.add(bl["link"].rstrip("/"))
    KNOWN_URLS = urls

    # Refer√™ncias legislativas
    laws = set()
    for fonte in data.get("fontes", []):
        nome = fonte.get("nome", "").lower()
        nome_norm = re.sub(r"(\d)\.(\d)", r"\1\2", nome)
        laws.add(nome_norm)
    for cat in data.get("categorias", []):
        for bl in cat.get("base_legal", []):
            lei = bl.get("lei", "").lower()
            lei_norm = re.sub(r"(\d)\.(\d)", r"\1\2", lei)
            laws.add(lei_norm)
    KNOWN_LAWS = laws

    return data


# ‚îÄ‚îÄ‚îÄ Fonte 1: DOU (Di√°rio Oficial da Uni√£o) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_dou(since_days: int = 60) -> list[dict]:
    """
    Busca publica√ß√µes oficiais e not√≠cias sobre PcD usando o motor de
    busca universal do portal gov.br:
    https://www.gov.br/pt-br/@@search?SearchableText=...

    Esse endpoint agrega conte√∫do de todo o governo federal ‚Äî DOU,
    servi√ßos, not√≠cias ‚Äî com filtros por tipo_conteudo (Not√≠cia,
    Servi√ßo, etc.) e pagina√ß√£o via start/b_size.
    """
    candidates = []
    log("Consultando gov.br @@search (busca universal ‚Äî DOU + not√≠cias)...")

    today = datetime.now().strftime("%Y-%m-%d")

    # Busca por cada keyword nas categorias Not√≠cia e geral
    for query in SEARCH_QUERIES[:3]:
        for tipo in ["Not√≠cia", ""]:
            encoded = urllib.parse.quote(query)
            params = f"SearchableText={encoded}&b_size=20"
            if tipo:
                params += f"&tipo_conteudo={urllib.parse.quote(tipo)}"
            url = f"https://www.gov.br/pt-br/@@search?{params}"

            label = f"({tipo or 'Todos'})"
            log(f"  gov.br busca '{query}' {label}...")
            html = fetch(url)
            if not html:
                continue

            # Extrai links de resultados ‚Äî t√≠tulos linkados do gov.br
            results = re.findall(
                r'<a[^>]+href="(https://www\.gov\.br/[^"]+)"[^>]*>\s*'
                r'(?:<[^>]+>)*([^<]{10,300})',
                html,
                re.IGNORECASE,
            )

            if not results:
                # Alternativo: qualquer link gov.br com texto significativo
                results = re.findall(
                    r'href="(https://www\.gov\.br/[^"]+)"[^>]*>([^<]{10,300})',
                    html,
                    re.IGNORECASE,
                )

            for result_url, title in results[:15]:
                title = title.strip()
                if not title or len(title) < 10:
                    continue

                # Ignora links de navega√ß√£o, footer, etc.
                skip_patterns = [
                    "/@@search", "/sitemap", "/termos-de-uso",
                    "/por-dentro-do-govbr", "/navegacao",
                    "/canais-do-executivo", "/orgaos-do-governo",
                    "vlibras.gov.br", "/apps/",
                ]
                if any(p in result_url.lower() for p in skip_patterns):
                    continue

                if is_known_url(result_url):
                    continue

                keywords = text_has_pcd_keywords(title)
                if keywords:
                    item_type = "news" if tipo == "Not√≠cia" else "legislation"
                    candidates.append({
                        "source": "gov.br Busca",
                        "title": title[:200],
                        "url": result_url,
                        "keywords": keywords[:5],
                        "date_found": today,
                        "query": query,
                        "type": item_type,
                        "hash": content_hash(result_url + title),
                    })

    # Dedup intra-busca
    seen: set[str] = set()
    unique = []
    for c in candidates:
        if c["hash"] not in seen:
            seen.add(c["hash"])
            unique.append(c)
    candidates = unique

    log(f"  gov.br Busca: {len(candidates)} candidatos encontrados", "OK" if candidates else "INFO")
    return candidates


# ‚îÄ‚îÄ‚îÄ Fonte 2: Senado Federal Dados Abertos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_senado(since_days: int = 60) -> list[dict]:
    """
    Consulta a API do Senado Federal (Dados Abertos) para mat√©rias
    legislativas recentes relacionadas a PcD.
    Endpoint: /dadosabertos/materia/pesquisa/lista
    Docs: https://legis.senado.leg.br/dadosabertos/
    """
    candidates = []
    log("Consultando Senado Federal Dados Abertos...")

    since_dt = datetime.now() - timedelta(days=since_days)
    since_yyyymmdd = since_dt.strftime("%Y%m%d")
    today_yyyymmdd = datetime.now().strftime("%Y%m%d")
    today = datetime.now().strftime("%Y-%m-%d")

    for query in SEARCH_QUERIES[:3]:
        encoded = urllib.parse.quote(query)
        url = (
            f"https://legis.senado.leg.br/dadosabertos/materia/pesquisa/lista"
            f"?palavraChave={encoded}"
            f"&dataInicioApresentacao={since_yyyymmdd}"
            f"&dataFimApresentacao={today_yyyymmdd}"
        )

        log(f"  Senado busca: '{query}'")
        data = fetch(url, json_resp=True)
        if not data:
            continue

        # Navega a estrutura do JSON: PesquisaBasicaMateria > Materias > Materia[]
        items = []
        if isinstance(data, dict):
            pesquisa = data.get("PesquisaBasicaMateria", data)
            materias = pesquisa.get("Materias", {})
            if isinstance(materias, dict):
                items = materias.get("Materia", [])
            elif isinstance(materias, list):
                items = materias

        if not isinstance(items, list):
            items = [items] if items else []

        for item in items[:10]:
            if not isinstance(item, dict):
                continue

            # Campos da mat√©ria
            ident = item.get("IdentificacaoMateria", {})
            if isinstance(ident, dict):
                sigla = ident.get("SiglaSubtipoMateria", "")
                numero = ident.get("NumeroMateria", "")
                ano = ident.get("AnoMateria", "")
            else:
                sigla = numero = ano = ""

            ementa = item.get("EmentaMateria", item.get("Ementa", "")).strip()
            if not ementa:
                # Tenta DadosBasicosMateria
                dados = item.get("DadosBasicosMateria", {})
                if isinstance(dados, dict):
                    ementa = dados.get("EmentaMateria", "").strip()

            if not ementa:
                continue

            law_ref = f"{sigla} {numero}/{ano}".strip()
            materia_url = f"https://www25.senado.leg.br/web/atividade/materias/-/materia/{numero}" if numero else ""

            if is_known_law(law_ref) or is_known_url(materia_url):
                continue

            keywords = text_has_pcd_keywords(ementa)
            if keywords:
                candidates.append({
                    "source": "Senado",
                    "title": f"{law_ref} ‚Äî {ementa[:120]}",
                    "url": materia_url,
                    "keywords": keywords[:5],
                    "date_found": today,
                    "query": query,
                    "type": "legislation",
                    "law_ref": law_ref,
                    "hash": content_hash(law_ref + ementa),
                })

    log(f"  Senado: {len(candidates)} candidatos encontrados", "OK" if candidates else "INFO")
    return candidates


# ‚îÄ‚îÄ‚îÄ Fonte 3: Portal gov.br Servi√ßos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_govbr_services() -> list[dict]:
    """
    Consulta o cat√°logo de servi√ßos do portal gov.br para novos
    servi√ßos digitais relevantes para PcD.
    """
    candidates = []
    log("Consultando cat√°logo de servi√ßos gov.br...")

    today = datetime.now().strftime("%Y-%m-%d")

    for query in SEARCH_QUERIES[:3]:
        encoded = urllib.parse.quote(query)
        # API p√∫blica do portal de servi√ßos
        url = (
            f"https://www.gov.br/pt-br/servicos/"
            f"@@search?SearchableText={encoded}"
            f"&portal_type=Servico"
        )

        log(f"  gov.br servi√ßos: '{query}'")
        html = fetch(url)
        if not html:
            continue

        # Parse resultados HTML
        results = re.findall(
            r'<a[^>]+href="(https://www\.gov\.br/[^"]+/servicos/[^"]+)"[^>]*>\s*'
            r'(?:<[^>]+>)*([^<]+)',
            html,
            re.IGNORECASE,
        )

        if not results:
            # Padr√£o alternativo
            results = re.findall(
                r'href="(https://www\.gov\.br/[^"]+)"[^>]*class="[^"]*titulo[^"]*"[^>]*>([^<]+)',
                html,
                re.IGNORECASE,
            )

        for service_url, title in results[:8]:
            title = title.strip()
            if not title or len(title) < 5:
                continue

            if is_known_url(service_url):
                continue

            keywords = text_has_pcd_keywords(title)
            if keywords:
                candidates.append({
                    "source": "gov.br",
                    "title": title[:150],
                    "url": service_url,
                    "keywords": keywords[:5],
                    "date_found": today,
                    "query": query,
                    "type": "service",
                    "hash": content_hash(service_url + title),
                })

    log(f"  gov.br servi√ßos: {len(candidates)} candidatos", "OK" if candidates else "INFO")
    return candidates


# ‚îÄ‚îÄ‚îÄ Fonte 4: Not√≠cias INSS / MDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_govbr_news() -> list[dict]:
    """
    Verifica not√≠cias recentes nos portais do INSS e MDS que possam
    indicar novos benef√≠cios ou mudan√ßas significativas.
    """
    candidates = []
    log("Consultando not√≠cias gov.br (INSS, MDS, Sa√∫de)...")

    today = datetime.now().strftime("%Y-%m-%d")

    news_sources = [
        ("INSS", "https://www.gov.br/inss/pt-br/noticias"),
        ("MDS", "https://www.gov.br/mds/pt-br/noticias-e-conteudos/noticias"),
        ("Sa√∫de", "https://www.gov.br/saude/pt-br/assuntos/noticias"),
        ("Direitos Humanos", "https://www.gov.br/mdh/pt-br/assuntos/noticias"),
    ]

    for source_name, news_url in news_sources:
        log(f"  Not√≠cias {source_name}...")
        html = fetch(news_url)
        if not html:
            continue

        # Extrai links de not√≠cias (aceita /noticias/ e /noticias-e-conteudos/)
        news_links = re.findall(
            r'<a[^>]+href="(https://www\.gov\.br/[^"]+/noticias[^"]*)"[^>]*>\s*'
            r'(?:<[^>]+>)*([^<]{10,200})',
            html,
            re.IGNORECASE,
        )

        if not news_links:
            news_links = re.findall(
                r'href="([^"]+)"[^>]*>([^<]*(?:defici|autis|pcd|bpc|acessibil|inclus√£o)[^<]*)',
                html,
                re.IGNORECASE,
            )

        for link_url, title in news_links[:10]:
            title = title.strip()
            if not title or len(title) < 10:
                continue

            if not link_url.startswith("http"):
                link_url = f"https://www.gov.br{link_url}"

            keywords = text_has_pcd_keywords(title)
            if keywords:
                candidates.append({
                    "source": f"Not√≠cias {source_name}",
                    "title": title[:150],
                    "url": link_url,
                    "keywords": keywords[:5],
                    "date_found": today,
                    "type": "news",
                    "hash": content_hash(link_url),
                })

    log(f"  Not√≠cias: {len(candidates)} candidatos", "OK" if candidates else "INFO")
    return candidates


# ‚îÄ‚îÄ‚îÄ Fonte 5: LexML Brasil ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_lexml(since_days: int = 60) -> list[dict]:
    """
    Consulta o portal LexML Brasil para legisla√ß√£o relacionada a PcD.
    Usa a busca web: https://www.lexml.gov.br/busca/search?keyword=...
    O LexML agrega 85 mil+ documentos legais de tribunais, Senado,
    C√¢mara, AGU e bibliotecas jur√≠dicas.
    """
    candidates = []
    log("Consultando LexML Brasil (busca web)...")

    today = datetime.now().strftime("%Y-%m-%d")

    # Termos de busca focados em legisla√ß√£o PcD
    kw_queries = [
        "pessoa com defici√™ncia",
        "autismo",
        "acessibilidade",
    ]

    for keyword in kw_queries:
        encoded = urllib.parse.quote(keyword)
        url = f"https://www.lexml.gov.br/busca/search?keyword={encoded}"

        log(f"  LexML busca: '{keyword}'")
        html = fetch(url)
        if not html:
            continue

        # Extrai resultados: links para URNs legislativas
        # Padr√£o: <a href="https://www.lexml.gov.br/urn/...">T√≠tulo</a>
        results = re.findall(
            r'<a[^>]+href="(https://www\.lexml\.gov\.br/urn/[^"]+)"[^>]*>\s*'
            r'(?:<[^>]+>)*\s*([^<]{10,300})',
            html,
            re.IGNORECASE,
        )

        if not results:
            # Alternativo: links com urn:lex no texto
            results = re.findall(
                r'href="(https://www\.lexml\.gov\.br/urn/urn[^"]+)"[^>]*>([^<]+)',
                html,
                re.IGNORECASE,
            )

        # Extrai tamb√©m ementas associadas (texto ap√≥s o link)
        # O LexML mostra "Ementa: texto..." perto dos links
        ementas = re.findall(
            r'(?:Ementa|ementa)\s*(?:</[^>]+>\s*)*([^<]{20,500})',
            html,
            re.IGNORECASE,
        )

        for i, (lex_url, title) in enumerate(results[:10]):
            title = title.strip()
            if not title or len(title) < 5:
                continue

            # Enriquece com ementa se dispon√≠vel
            ementa = ementas[i].strip() if i < len(ementas) else ""
            full_title = title
            if ementa and len(ementa) > len(title):
                full_title = f"{title} ‚Äî {ementa[:120]}"

            if is_known_url(lex_url) or is_known_law(title):
                continue

            search_text = f"{title} {ementa}"
            keywords = text_has_pcd_keywords(search_text)
            if keywords:
                candidates.append({
                    "source": "LexML",
                    "title": full_title[:200],
                    "url": lex_url,
                    "keywords": keywords[:5],
                    "date_found": today,
                    "type": "legislation",
                    "hash": content_hash(lex_url + title),
                })

    # Dedup
    seen: set[str] = set()
    unique = []
    for c in candidates:
        if c["hash"] not in seen:
            seen.add(c["hash"])
            unique.append(c)
    candidates = unique

    log(f"  LexML: {len(candidates)} candidatos", "OK" if candidates else "INFO")
    return candidates


# ‚îÄ‚îÄ‚îÄ Fonte 6: Portais Estaduais (27 UFs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_estados(since_days: int = 60, filtro_uf: list[str] | None = None) -> list[dict]:
    """
    Busca PcD em n√≠vel estadual usando gov.br @@search com nome do estado.
    Cobre todos os 27 estados + DF. Apenas URLs de dom√≠nios oficiais.
    """
    candidates = []
    estados = {uf: nome for uf, nome in UF_ESTADOS.items()
               if not filtro_uf or uf in filtro_uf}
    log(f"Consultando fontes estaduais ({len(estados)} UFs)...")
    today = datetime.now().strftime("%Y-%m-%d")

    skip_patterns = [
        "/@@search", "/sitemap", "/termos-de-uso",
        "/por-dentro-do-govbr", "/navegacao",
        "/canais-do-executivo", "/orgaos-do-governo",
        "vlibras.gov.br", "/apps/",
    ]

    for uf, estado in estados.items():
        search_term = f"pessoa com defici√™ncia {estado}"
        encoded = urllib.parse.quote(search_term)
        url = f"https://www.gov.br/pt-br/@@search?SearchableText={encoded}&b_size=10"

        log(f"  {uf} ({estado})...")
        html = fetch(url)
        if not html:
            continue

        results = re.findall(
            r'<a[^>]+href="(https://www\.gov\.br/[^"]+)"[^>]*>\s*'
            r'(?:<[^>]+>)*([^<]{10,300})',
            html, re.IGNORECASE,
        )
        if not results:
            results = re.findall(
                r'href="(https://www\.gov\.br/[^"]+)"[^>]*>([^<]{10,300})',
                html, re.IGNORECASE,
            )

        for result_url, title in results[:8]:
            title = title.strip()
            if not title or len(title) < 10:
                continue
            if any(p in result_url.lower() for p in skip_patterns):
                continue
            if is_known_url(result_url) or not is_official_domain(result_url):
                continue

            keywords = text_has_pcd_keywords(title)
            if keywords:
                candidates.append({
                    "source": f"Estado {uf}",
                    "title": title[:200],
                    "url": result_url,
                    "keywords": keywords[:5],
                    "date_found": today,
                    "query": search_term,
                    "type": "legislation",
                    "nivel": "estadual",
                    "uf": uf,
                    "hash": content_hash(result_url + title),
                })

    # Dedup intra-busca
    seen: set[str] = set()
    unique = []
    for c in candidates:
        if c["hash"] not in seen:
            seen.add(c["hash"])
            unique.append(c)

    log(f"  Estados: {len(unique)} candidatos", "OK" if unique else "INFO")
    return unique


# ‚îÄ‚îÄ‚îÄ Fonte 7: Portais Municipais ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_municipios(since_days: int = 60, filtro_cidades: list[str] | None = None) -> list[dict]:
    """
    Busca PcD em n√≠vel municipal via gov.br @@search + portais configurados.
    Apenas URLs de dom√≠nios oficiais (DOMINIOS_OFICIAIS).
    """
    candidates = []
    cidades = {k: v for k, v in MUNICIPIOS_BR.items()
               if not filtro_cidades or k in filtro_cidades}

    log(f"Consultando fontes municipais ({len(cidades)} cidades)...")
    today = datetime.now().strftime("%Y-%m-%d")

    skip_nav = [
        "/@@search", "/sitemap", "/termos-de-uso",
        "/por-dentro-do-govbr", "/navegacao",
        "/canais-do-executivo", "/orgaos-do-governo",
        "vlibras.gov.br", "/apps/",
    ]
    skip_portal = [
        "/@@", "/sitemap", "/login", "/cadastro",
        "javascript:", "#", ".pdf", ".jpg", ".png",
        "vlibras.gov.br",
    ]

    for slug, info in cidades.items():
        nome = info["nome"]
        uf = info["uf"]

        # --- Estrat√©gia 1: gov.br @@search com nome da cidade ---
        search_term = f"pessoa com defici√™ncia {nome} {uf}"
        encoded = urllib.parse.quote(search_term)
        url = f"https://www.gov.br/pt-br/@@search?SearchableText={encoded}&b_size=10"

        log(f"  {nome}/{uf} (gov.br)...")
        html = fetch(url)
        if html:
            results = re.findall(
                r'<a[^>]+href="(https://www\.gov\.br/[^"]+)"[^>]*>\s*'
                r'(?:<[^>]+>)*([^<]{10,300})',
                html, re.IGNORECASE,
            )
            if not results:
                results = re.findall(
                    r'href="(https://www\.gov\.br/[^"]+)"[^>]*>([^<]{10,300})',
                    html, re.IGNORECASE,
                )

            for result_url, title in results[:5]:
                title = title.strip()
                if not title or len(title) < 10:
                    continue
                if any(p in result_url.lower() for p in skip_nav):
                    continue
                if is_known_url(result_url) or not is_official_domain(result_url):
                    continue

                keywords = text_has_pcd_keywords(title)
                if keywords:
                    candidates.append({
                        "source": f"Munic√≠pio {nome}/{uf}",
                        "title": title[:200],
                        "url": result_url,
                        "keywords": keywords[:5],
                        "date_found": today,
                        "query": search_term,
                        "type": "service",
                        "nivel": "municipal",
                        "uf": uf,
                        "municipio": nome,
                        "hash": content_hash(result_url + title),
                    })

        # --- Estrat√©gia 2: Scraping de portais municipais PcD ---
        portal = info.get("portal", "")
        for pagina_url in info.get("paginas_pcd", []):
            log(f"  {nome}/{uf} (portal PcD)...")
            phtml = fetch(pagina_url)
            if not phtml:
                continue

            portal_links = re.findall(
                r'href="([^"]+)"[^>]*>([^<]{5,200})',
                phtml, re.IGNORECASE,
            )

            for link_url, title in portal_links:
                title = title.strip()
                if not title or len(title) < 5:
                    continue
                # Normaliza URL relativa
                if link_url.startswith("/"):
                    link_url = f"{portal.rstrip('/')}{link_url}"
                elif not link_url.startswith("http"):
                    continue
                if not is_official_domain(link_url):
                    continue
                if is_known_url(link_url):
                    continue
                if any(p in link_url.lower() for p in skip_portal):
                    continue

                keywords = text_has_pcd_keywords(title)
                if keywords:
                    candidates.append({
                        "source": f"Portal {nome}/{uf}",
                        "title": title[:200],
                        "url": link_url,
                        "keywords": keywords[:5],
                        "date_found": today,
                        "type": "service",
                        "nivel": "municipal",
                        "uf": uf,
                        "municipio": nome,
                        "hash": content_hash(link_url + title),
                    })

    # Dedup
    seen: set[str] = set()
    unique = []
    for c in candidates:
        if c["hash"] not in seen:
            seen.add(c["hash"])
            unique.append(c)

    log(f"  Munic√≠pios: {len(unique)} candidatos", "OK" if unique else "INFO")
    return unique


# ‚îÄ‚îÄ‚îÄ Deduplica√ß√£o e Classifica√ß√£o ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def deduplicate(candidates: list[dict]) -> list[dict]:
    """Remove duplicatas por hash e URL."""
    seen_hashes: set[str] = set()
    seen_urls: set[str] = set()
    unique = []

    for c in candidates:
        h = c.get("hash", "")
        u = c.get("url", "").rstrip("/")
        if h in seen_hashes or u in seen_urls:
            continue
        seen_hashes.add(h)
        seen_urls.add(u)
        unique.append(c)

    return unique


def classify_priority(candidate: dict) -> str:
    """Classifica prioridade: ALTA, M√âDIA, BAIXA."""
    title_lower = candidate.get("title", "").lower()
    kw_count = len(candidate.get("keywords", []))
    source = candidate.get("source", "")

    # Alta: legisla√ß√£o nova + m√∫ltiplas keywords + fonte priorit√°ria
    if candidate.get("type") == "legislation" and kw_count >= 3:
        return "ALTA"
    if any(term in title_lower for term in ["lei", "decreto", "bpc", "loas", "lbi"]):
        return "ALTA"

    # M√©dia: servi√ßos novos ou not√≠cias relevantes
    if source in ("gov.br", "Senado") or kw_count >= 2:
        return "M√âDIA"

    return "BAIXA"


def classify_category(candidate: dict) -> str:
    """Sugere a categoria mais relevante do direitos.json."""
    title_lower = candidate.get("title", "").lower()
    keywords_lower = [kw.lower() for kw in candidate.get("keywords", [])]
    all_text = title_lower + " " + " ".join(keywords_lower)

    mapping = {
        "bpc": ["bpc", "loas", "benef√≠cio assistencial", "presta√ß√£o continuada"],
        "educacao": ["educa√ß√£o", "escola", "prouni", "fies", "enem", "inclus√£o escolar"],
        "ciptea": ["ciptea", "autismo", "tea", "espectro autista", "romeo mion", "berenice piana"],
        "transporte": ["transporte", "passe livre", "mobilidade", "√¥nibus", "metr√¥"],
        "trabalho": ["trabalho", "emprego", "cota", "reserva de vagas", "pcd no trabalho"],
        "sus_terapias": ["sus", "terapia", "sa√∫de", "caps", "cer", "reabilita√ß√£o"],
        "plano_saude": ["plano de sa√∫de", "ans", "rol de procedimentos", "operadora"],
        "isencoes_tributarias": ["isen√ß√£o", "ipi", "icms", "iof", "ipva", "tribut"],
        "moradia": ["moradia", "minha casa", "habita√ß√£o", "acessibilidade residencial"],
        "fgts": ["fgts", "saque", "fundo de garantia"],
        "tecnologia_assistiva": ["tecnologia assistiva", "√≥rtese", "pr√≥tese", "cadeira de rodas"],
        "aposentadoria_especial_pcd": ["aposentadoria", "previd√™ncia", "inss"],
        "isencao_ir": ["imposto de renda", "ir", "isen√ß√£o ir"],
        "atendimento_prioritario": ["atendimento priorit√°rio", "prioridade", "fila"],
        "meia_entrada": ["meia-entrada", "meia entrada", "cultura", "lazer"],
        "tarifa_social_energia": ["tarifa social", "energia", "luz"],
        "auxilio_inclusao": ["aux√≠lio-inclus√£o", "aux√≠lio inclus√£o"],
        "estacionamento_especial": ["estacionamento", "vaga especial"],
    }

    for cat_id, terms in mapping.items():
        if any(term in all_text for term in terms):
            return cat_id

    return "nova_categoria"


# ‚îÄ‚îÄ‚îÄ Gera√ß√£o de Relat√≥rios ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def generate_report(candidates: list[dict], existing_data: dict, since_days: int) -> dict:
    """Gera relat√≥rio estruturado com candidatos classificados."""
    report = {
        "generated_at": datetime.now().isoformat(),
        "search_window_days": since_days,
        "existing_categories": len(existing_data.get("categorias", [])),
        "existing_sources": len(existing_data.get("fontes", [])),
        "existing_urls": len(KNOWN_URLS),
        "candidates_found": len(candidates),
        "candidates_by_priority": {"ALTA": 0, "M√âDIA": 0, "BAIXA": 0},
        "candidates_by_source": {},
        "candidates_by_type": {},
        "candidates": [],
    }

    for c in candidates:
        priority = classify_priority(c)
        category = classify_category(c)
        c["priority"] = priority
        c["suggested_category"] = category

        report["candidates_by_priority"][priority] = report["candidates_by_priority"].get(priority, 0) + 1
        report["candidates_by_source"][c["source"]] = report["candidates_by_source"].get(c["source"], 0) + 1
        report["candidates_by_type"][c["type"]] = report["candidates_by_type"].get(c["type"], 0) + 1
        report["candidates"].append(c)

    # Ordena: ALTA primeiro, depois M√âDIA, depois BAIXA
    priority_order = {"ALTA": 0, "M√âDIA": 1, "BAIXA": 2}
    report["candidates"].sort(key=lambda c: priority_order.get(c.get("priority", "BAIXA"), 3))

    return report


def generate_markdown(report: dict) -> str:
    """Gera relat√≥rio Markdown leg√≠vel."""
    lines = [
        f"# üîç NossoDireito ‚Äî Relat√≥rio de Descoberta de Novos Benef√≠cios",
        f"",
        f"**Gerado em:** {report['generated_at'][:19]}",
        f"**Janela de busca:** √∫ltimos {report['search_window_days']} dias",
        f"**Categorias existentes:** {report['existing_categories']}",
        f"**Fontes existentes:** {report['existing_sources']}",
        f"**Candidatos encontrados:** {report['candidates_found']}",
        f"",
    ]

    if not report["candidates"]:
        lines.append("‚úÖ Nenhum benef√≠cio novo detectado. Base de dados est√° atualizada!")
        lines.append("")
        lines.append("*Nota: Isso n√£o garante que n√£o existam novos benef√≠cios. A verifica√ß√£o")
        lines.append("manual peri√≥dica continua necess√°ria conforme GOVERNANCE.md.*")
        return "\n".join(lines)

    # Resumo por prioridade
    lines.append("## üìä Resumo")
    lines.append("")
    lines.append("| Prioridade | Quantidade |")
    lines.append("|------------|------------|")
    for p in ["ALTA", "M√âDIA", "BAIXA"]:
        count = report["candidates_by_priority"].get(p, 0)
        icon = "üî¥" if p == "ALTA" else "üü°" if p == "M√âDIA" else "üü¢"
        lines.append(f"| {icon} {p} | {count} |")
    lines.append("")

    # Resumo por fonte
    lines.append("| Fonte | Candidatos |")
    lines.append("|-------|------------|")
    for src, count in sorted(report["candidates_by_source"].items()):
        lines.append(f"| {src} | {count} |")
    lines.append("")

    # Detalhes dos candidatos
    lines.append("## üÜï Candidatos para Revis√£o")
    lines.append("")

    current_priority = None
    for i, c in enumerate(report["candidates"], 1):
        if c["priority"] != current_priority:
            current_priority = c["priority"]
            icon = "üî¥" if current_priority == "ALTA" else "üü°" if current_priority == "M√âDIA" else "üü¢"
            lines.append(f"### {icon} Prioridade {current_priority}")
            lines.append("")

        lines.append(f"**{i}. {c['title']}**")
        lines.append(f"- **Fonte:** {c['source']}")
        lines.append(f"- **Tipo:** {c['type']}")
        lines.append(f"- **URL:** {c['url']}")
        lines.append(f"- **Keywords:** {', '.join(c['keywords'])}")
        lines.append(f"- **Categoria sugerida:** `{c.get('suggested_category', 'N/A')}`")
        lines.append(f"- **Encontrado em:** {c['date_found']}")
        lines.append("")

    # A√ß√µes recomendadas
    lines.extend([
        "## ‚úÖ A√ß√µes Recomendadas",
        "",
        "Para cada candidato de prioridade **ALTA**:",
        "1. Verificar se √© realmente um novo direito/benef√≠cio PcD",
        "2. Validar a fonte oficial (planalto.gov.br, gov.br)",
        "3. Adicionar a `data/direitos.json` seguindo o schema",
        "4. Executar `python scripts/validate_all.py` para validar",
        "5. Atualizar `ultima_atualizacao` em `direitos.json`",
        "",
        "---",
        "*Relat√≥rio gerado automaticamente por `scripts/discover_benefits.py`.*",
        "*Consulte GOVERNANCE.md para crit√©rios de adi√ß√£o de novas categorias.*",
    ])

    return "\n".join(lines)


def generate_github_issue_body(report: dict) -> str:
    """Gera corpo de issue GitHub para revis√£o."""
    md = generate_markdown(report)

    # Adiciona checklist interativo para issues
    candidates = report.get("candidates", [])
    if candidates:
        checklist = ["\n## Checklist de Revis√£o\n"]
        for i, c in enumerate(candidates, 1):
            icon = "üî¥" if c["priority"] == "ALTA" else "üü°" if c["priority"] == "M√âDIA" else "üü¢"
            checklist.append(f"- [ ] {icon} **{c['title'][:80]}** ({c['source']})")
        md += "\n" + "\n".join(checklist)

    return md


# ‚îÄ‚îÄ‚îÄ Merge com relat√≥rio anterior ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def merge_with_previous(report: dict) -> dict:
    """
    Combina novos candidatos com relat√≥rio anterior, evitando
    re-reportar itens j√° analisados.
    """
    if not REPORT_JSON.exists():
        return report

    try:
        with open(REPORT_JSON, "r", encoding="utf-8") as f:
            previous = json.load(f)
    except (json.JSONDecodeError, OSError):
        return report

    # Hashes j√° reportados anteriormente
    previous_hashes = {c.get("hash") for c in previous.get("candidates", [])}

    # Filtra apenas candidatos realmente novos
    new_candidates = [
        c for c in report["candidates"]
        if c.get("hash") not in previous_hashes
    ]

    if len(new_candidates) < len(report["candidates"]):
        removed = len(report["candidates"]) - len(new_candidates)
        log(f"  {removed} candidatos j√° reportados anteriormente (filtrados)", "SKIP")

    report["candidates"] = new_candidates
    report["candidates_found"] = len(new_candidates)

    # Recalcula contadores
    report["candidates_by_priority"] = {"ALTA": 0, "M√âDIA": 0, "BAIXA": 0}
    report["candidates_by_source"] = {}
    report["candidates_by_type"] = {}
    for c in new_candidates:
        p = c.get("priority", "BAIXA")
        report["candidates_by_priority"][p] = report["candidates_by_priority"].get(p, 0) + 1
        src = c.get("source", "?")
        report["candidates_by_source"][src] = report["candidates_by_source"].get(src, 0) + 1
        t = c.get("type", "?")
        report["candidates_by_type"][t] = report["candidates_by_type"].get(t, 0) + 1

    return report


# ‚îÄ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main():
    parser = argparse.ArgumentParser(
        description="Descoberta autom√°tica de novos benef√≠cios PcD"
    )
    parser.add_argument("--since", type=int, default=60,
                        help="Buscar nos √∫ltimos N dias (padr√£o: 60)")
    parser.add_argument("--dry-run", action="store_true",
                        help="Apenas mostra o que faria, sem salvar")
    parser.add_argument("--json", action="store_true",
                        help="Sa√≠da apenas em JSON (para CI)")
    parser.add_argument("--no-merge", action="store_true",
                        help="N√£o mesclar com relat√≥rio anterior")
    parser.add_argument("--nivel", choices=["federal", "estadual", "municipal", "todos"],
                        default="todos",
                        help="N√≠vel de busca (padr√£o: todos)")
    parser.add_argument("--estado", nargs="*", metavar="UF",
                        help="Filtrar UF(s): --estado SP RJ MG")
    parser.add_argument("--cidade", nargs="*", metavar="SLUG",
                        help="Filtrar munic√≠pios: --cidade barueri-sp")
    args = parser.parse_args()

    print("=" * 70)
    print("üîç NOSSODIREITO ‚Äî DESCOBERTA AUTOM√ÅTICA DE BENEF√çCIOS PcD")
    print("=" * 70)
    print(f"üìÖ Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÜ Janela de busca: √∫ltimos {args.since} dias")
    print(f"üåé N√≠vel: {args.nivel.upper()}")
    print(f"üìñ Dicion√°rio PcD: {'carregado' if DICT_FILE.exists() else 'n√£o encontrado'} ({len(PCD_KEYWORDS)} keywords, {len(SEARCH_QUERIES)} queries)")
    if args.estado:
        print(f"  UFs: {', '.join(args.estado)}")
    if args.cidade:
        print(f"  Cidades: {', '.join(args.cidade)}")
    print()

    # 1. Carregar dados atuais
    log("Carregando dados existentes de direitos.json...")
    existing_data = load_existing_data()
    log(f"  {len(KNOWN_IDS)} categorias, {len(KNOWN_URLS)} URLs, {len(KNOWN_LAWS)} leis conhecidas", "OK")
    print()

    # 2. Consultar todas as fontes
    all_candidates = []
    sources = []

    # N√≠vel federal (5 fontes)
    if args.nivel in ("federal", "todos"):
        sources.extend([
            ("DOU", lambda: search_dou(args.since)),
            ("Senado", lambda: search_senado(args.since)),
            ("gov.br Servi√ßos", search_govbr_services),
            ("gov.br Not√≠cias", search_govbr_news),
            ("LexML", lambda: search_lexml(args.since)),
        ])

    # N√≠vel estadual (27 UFs)
    if args.nivel in ("estadual", "todos"):
        filtro_uf = [u.upper() for u in args.estado] if args.estado else None
        sources.append(("Estados", lambda fu=filtro_uf: search_estados(args.since, fu)))

    # N√≠vel municipal (cidades configuradas)
    if args.nivel in ("municipal", "todos"):
        filtro_cidades = args.cidade if args.cidade else None
        sources.append(("Munic√≠pios", lambda fc=filtro_cidades: search_municipios(args.since, fc)))

    for name, search_fn in sources:
        try:
            results = search_fn()
            all_candidates.extend(results)
        except Exception as e:
            log(f"Erro na fonte {name}: {e}", "ERR")
        print()

    # Normalizar n√≠vel para candidatos federais existentes
    for c in all_candidates:
        if "nivel" not in c:
            c["nivel"] = "federal"

    # 3. Deduplicar
    unique = deduplicate(all_candidates)
    if len(unique) < len(all_candidates):
        log(f"Deduplica√ß√£o: {len(all_candidates)} ‚Üí {len(unique)} candidatos", "INFO")

    # 3b. Validar dom√≠nios oficiais
    validated = [c for c in unique if is_official_domain(c.get("url", ""))]
    if len(validated) < len(unique):
        removed = len(unique) - len(validated)
        log(f"Valida√ß√£o: {removed} URLs de fontes n√£o-oficiais removidas", "WARN")
    unique = validated

    # 4. Gerar relat√≥rio
    report = generate_report(unique, existing_data, args.since)

    # 5. Merge com relat√≥rio anterior
    if not args.no_merge:
        report = merge_with_previous(report)

    # 6. Sa√≠da
    print("=" * 70)
    print("üìä RESULTADOS DA DESCOBERTA")
    print("=" * 70)
    print()

    if args.json:
        print(json.dumps(report, ensure_ascii=False, indent=2))
    else:
        md = generate_markdown(report)
        print(md)

    # 7. Salvar relat√≥rios
    if not args.dry_run:
        REPORT_DIR.mkdir(exist_ok=True)

        with open(REPORT_JSON, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        log(f"Relat√≥rio JSON salvo: {REPORT_JSON.relative_to(ROOT)}", "OK")

        with open(REPORT_MD, "w", encoding="utf-8") as f:
            f.write(generate_markdown(report))
        log(f"Relat√≥rio MD salvo: {REPORT_MD.relative_to(ROOT)}", "OK")
    else:
        log("Modo dry-run ‚Äî relat√≥rios n√£o salvos", "WARN")

    print()
    total = report["candidates_found"]
    alta = report["candidates_by_priority"].get("ALTA", 0)
    if total == 0:
        log("Nenhum benef√≠cio novo detectado. Base atualizada! ‚úÖ", "OK")
        return 0
    else:
        log(f"{total} candidatos encontrados ({alta} prioridade ALTA)", "NEW")
        return 1 if alta > 0 else 0


if __name__ == "__main__":
    sys.exit(main())
